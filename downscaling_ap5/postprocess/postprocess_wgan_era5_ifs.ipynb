{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185e6a0b-fe7d-4727-9e66-69debc18bbcd",
   "metadata": {},
   "source": [
    "# Postprocessing trained downscaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303bec6-8cc5-46ee-8775-3f98038fcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../models/\")\n",
    "sys.path.append(\"../utils/\")\n",
    "sys.path.append(\"../handle_data/\")\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from handle_data_unet import *\n",
    "from handle_data_class import  *\n",
    "from statistical_evaluation import Scores\n",
    "from plotting import *\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import json as js"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb03821-0602-453f-bbf6-7e2bd105ca84",
   "metadata": {},
   "source": [
    "## Base directories for test dataset and model\n",
    "\n",
    "Adapt `datadir`, `model_base_dir` and `model_name`.\n",
    " - `datadir`: directory where the test dataset is stored\n",
    " - `model_base_dir`: top-level directory where trained downscaling models are saved\n",
    " - `model_name`: name of trained model\n",
    " - `lztar`: flag if high-resolved (target) topography is part of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326facd-5f1f-4f74-861c-cff1ab255c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_era5_crea6/netcdf_data/all_files/\"\n",
    "model_base_dir = \"/p/home/jusers/langguth1/juwels/downscaling_maelstrom/downscaling_jsc_repo/downscaling_ap5/trained_models\"\n",
    "# name of the model to be postprocessed\n",
    "model_name = \"wgan_era5_to_crea6_epochs40_supervision_ztar2in_noes2\"\n",
    "lztar = True\n",
    "\n",
    "# constrct model directory paths\n",
    "model_base = os.path.join(model_base_dir, model_name)\n",
    "model_dir = os.path.join(model_base, f\"{model_name}_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d9ba3-19cc-4dcc-9af8-4c85508ba9c0",
   "metadata": {},
   "source": [
    "Next, we load the model and also retrieve the testing dataset by reading the corresponding netCDF-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f028ee9-b707-4292-ba45-ed231143392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Load model '{model_name}'\")\n",
    "trained_model = keras.models.load_model(model_dir, compile=False)\n",
    "print(f\"Read training dataset from {data_dir}\") \n",
    "ds_test = xr.open_dataset(os.path.join(data_dir, \"preproc_era5_crea6_test.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0f8af-2954-4048-bbb1-1445038a9325",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "After retrieving the reference data (i.e. the ground truth data)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d2954-d4a6-4832-8710-36d918efb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = ds_test[\"t_2m_tar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b567b0-dc0e-4cb5-87e0-b5449da3b06b",
   "metadata": {},
   "source": [
    "... we preprocess the input from the test dataset. For this, the data is reshaped into a xarray DataArray whose last dimension corresponds to the variables (the feature channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f4307-3dbb-4073-8001-9c425ee3bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalization parameters from saved json file\n",
    "js_norm = os.path.join(model_dir, \"..\", \"z_norm_dict.json\")\n",
    "\n",
    "try:\n",
    "    with open(js_norm, \"r\") as f:\n",
    "        norm_dict = js.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Could not find '{js_norm}'. Please check model-directory '{model_dir}'.\")\n",
    "\n",
    "train_vars = list(ds_test.keys())\n",
    "mu_train, std_train = np.asarray(norm_dict[\"mu\"]), np.asarray(norm_dict[\"std\"])\n",
    "da_test = HandleDataClass.reshape_ds(ds_test)\n",
    "da_test = HandleUnetData.z_norm_data(da_test, norm_method=\"norm\", save_path=model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2da370-7b1c-4fda-a4b2-2cb278d1db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the inputs and the target data\n",
    "da_test_in, da_test_tar = HandleDataClass.split_in_tar(da_test)\n",
    "if lztar:\n",
    "    print(\"Add high-resolved target topography to input features.\")\n",
    "    da_test_in = xr.concat([da_test_in, da_test_tar.sel({\"variables\": \"hsurf_tar\"})], dim=\"variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ecab4-c0eb-4519-ba27-637c81bdab14",
   "metadata": {},
   "source": [
    "## Create predictions from trained model\n",
    "\n",
    "The preprocessed data is fed into the trained model to obtain the downscalted 2m tmepertaure which is subject to evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68429da6-fc6e-4ec2-a400-4345e496c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start inference from trained model...\")\n",
    "y_pred_trans =  trained_model.predict(da_test_in.squeeze().values, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec3e28-72df-45b1-9c56-0651bbeb67c7",
   "metadata": {},
   "source": [
    "For evaluation, we have to denormalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4f144-588c-4357-8d10-33ece738f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates and dimensions from target data\n",
    "coords = da_test_tar.isel(variables=0).squeeze().coords\n",
    "dims = da_test_tar.isel(variables=0).squeeze().dims\n",
    "y_pred = xr.DataArray(y_pred_trans[0].squeeze(), coords=coords, dims=dims)\n",
    "# perform denormalization\n",
    "y_pred = HandleUnetData.denormalize(y_pred.squeeze(), \n",
    "                                    norm_dict[\"mu\"][\"t_2m_tar\"], \n",
    "                                    norm_dict[\"std\"][\"t_2m_tar\"])\n",
    "y_pred = xr.DataArray(y_pred, coords=coords, dims=dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cac8dc-cc46-4976-b3de-718cd740ce14",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Subsequently, the produced downscaling product is evaluated using the following scores\n",
    "- RMSE\n",
    "- Bias\n",
    "- Horizontal gradient ratio\n",
    "For this, we instantiate a score-engine which allows us to efficiently calculate some scores. Furthermore, we set and create the directory for saving the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbc447-29a8-428a-a93f-c2fff39cb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get plot directory\n",
    "plot_dir = os.path.join(\".\", model_name)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "avg_dims = [\"rlat\", \"rlon\"]\n",
    "# instantiate score engine\n",
    "score_engine = Scores(y_pred, ground_truth, avg_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ec108-d81c-401c-8d92-f9df5fd2a661",
   "metadata": {},
   "source": [
    "To run the evaluation and to create the desired plots, we define a small auxiliary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37a4c9-5933-43f8-996e-f9aaf767f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(score_engine, score_name: str, score_unit: str, **plt_kwargs):\n",
    "    print(f\"Start evaluation in terms of {score_name}\")\n",
    "    score_all = score_engine(score_name)\n",
    "    \n",
    "    score_hourly_all = score_all.groupby(\"time.hour\")\n",
    "    score_hourly_mean, score_hourly_std = score_hourly_all.mean(), score_hourly_all.std()\n",
    "    for hh in range(24):\n",
    "        if hh == 0:\n",
    "            tmp = score_all.isel({\"time\": score_all.time.dt.hour == hh}).groupby(\"time.season\")\n",
    "            score_hourly_mean_sea, score_hourly_std_sea = tmp.mean().copy(), tmp.std().copy()\n",
    "        else:\n",
    "            tmp = score_all.isel({\"time\": score_all.time.dt.hour == hh}).groupby(\"time.season\")\n",
    "            score_hourly_mean_sea, score_hourly_std_sea = xr.concat([score_hourly_mean_sea, tmp.mean()], dim=\"hour\"), \\\n",
    "                                                          xr.concat([score_hourly_std_sea, tmp.std()], dim=\"hour\")\n",
    "   \n",
    "    # create plots                                  \n",
    "    create_line_plot(score_hourly_mean, score_hourly_std, \"WGAN\",\n",
    "                     {score_name.upper(): score_unit}, os.path.join(plot_dir, f\"downscaling_wgan_{score_name.lower()}.png\"), **plt_kwargs)\n",
    "\n",
    "    for sea in score_hourly_mean_sea[\"season\"]:\n",
    "        create_line_plot(score_hourly_mean_sea.sel({\"season\": sea}), \n",
    "                         score_hourly_std_sea.sel({\"season\": sea}),\n",
    "                         \"WGAN\", {score_name.upper(): score_unit},\n",
    "                         os.path.join(plot_dir, f\"downscaling_wgan_{score_name.lower()}_{sea.values}.png\"), \n",
    "                         **plt_kwargs)\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd3eab-6874-44ef-b1ad-3fe6e5036cb2",
   "metadata": {},
   "source": [
    "Next, we perform the evaluation in terms of the desired metrics sequentially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c7cc4-fe45-4121-bc52-38291cc53739",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_evaluation(score_engine, \"rmse\", \"K\")\n",
    "_ = run_evaluation(score_engine, \"bias\", \"K\", value_range=(-2., 2.), ref_line=0.)\n",
    "_ = run_evaluation(score_engine, \"grad_amplitude\", \"1\", value_range=(0.8, 1.2), ref_line=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e7df2-e529-4f0c-9df0-ca5f188e5989",
   "metadata": {},
   "source": [
    "The evaluation aggregated over the whole target domain is complemented by a spatial ebaluation of the verification metrics.\n",
    "This is useful to identify regions where the downscaling model is most prone to errors and to underpin potential reasons for this behaviour. \n",
    "Thus, we initialize a new Scores-engine which does not perform any averaging beforehand (empty list passed as dims) and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ee2df-a81e-4125-b160-ebd0a34ee5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_engine = Scores(y_pred, ground_truth, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48310e-ed78-4af1-af6f-df97605f33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "... again create a small auxiliary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd24bd-c8de-4a5a-ba57-117af3316550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_spatial(score_engine, score_name: str, score_unit: str, plot_dir, **plt_kwargs):\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    score_all = score_engine(score_name)\n",
    "    cosmo_prj = ccrs.RotatedPole(pole_longitude=-162.0, pole_latitude=39.25)\n",
    "    \n",
    "    score_mean = score_all.mean(dim=\"time\")\n",
    "    fname = os.path.join(plot_dir, f\"downscaling_wgan_{score_name.lower()}_avg_map.png\") \n",
    "    create_map_score(score_mean, fname, score_dims = [\"rlat\", \"rlon\"],\n",
    "                     title=f\"{score_name.upper()} (avg.)\", projection=cosmo_prj, **plt_kwargs)    \n",
    "    \n",
    "    score_hourly_mean = score_all.groupby(\"time.hour\").mean(dim=[\"time\"])\n",
    "    for hh in range(24):   \n",
    "        fname = os.path.join(plot_dir, f\"downscaling_wgan_{score_name.lower()}_{hh:02d}_map.png\")                                  \n",
    "        create_map_score(score_hourly_mean.sel({\"hour\": hh}), fname, \n",
    "                         score_dims=[\"rlat\", \"rlon\"], title=f\"{score_name.upper()} {hh:02d} UTC\",\n",
    "                         projection=cosmo_prj, **plt_kwargs)\n",
    "\n",
    "    for hh in range(24):\n",
    "        score_now = score_all.isel({\"time\": score_all.time.dt.hour == hh}).groupby(\"time.season\").mean(dim=\"time\")\n",
    "        for sea in score_now[\"season\"]:\n",
    "            fname = os.path.join(plot_dir, f\"downscaling_wgan_{score_name.lower()}_{sea.values}_{hh:02d}_map.png\") \n",
    "            create_map_score(score_now.sel({\"season\": sea}), fname, score_dims = [\"rlat\", \"rlon\"],\n",
    "                             title=f\"{score_name} {sea.values} {hh:02d} UTC\", projection=cosmo_prj, **plt_kwargs)\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f2e3e-ba1d-453b-aaff-2b6c1b832288",
   "metadata": {},
   "source": [
    "We run the spatial evaluation procedure for the different metrics. Note that the plots are saved in separated sub-directories for better organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cedf003-1c9c-4fa8-b33b-0ab4f02151d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl_rmse = np.arange(0., 3.1, 0.2)\n",
    "cmap_rmse = mpl.cm.afmhot_r(np.linspace(0., 1., len(lvl_rmse)))\n",
    "_ = run_evaluation_spatial(score_engine, \"rmse\", \"K\", os.path.join(plot_dir, \"rmse_spatial\"), cmap=cmap_rmse, levels=lvl_rmse)\n",
    "\n",
    "lvl_bias = np.arange(-2., 2.1, 0.1)\n",
    "cmap_bias = mpl.cm.seismic(np.linspace(0., 1., len(lvl_bias)))\n",
    "_ = run_evaluation_spatial(score_engine, \"bias\", \"K\", os.path.join(plot_dir, \"bias_spatial\"), cmap=cmap_bias, levels=lvl_bias)\n",
    "\n",
    "# does not work as the gradient gets already spatially averaged \n",
    "#lvl_grad = np.arange(0.5, 1.51, 0.025)\n",
    "#cmap_grad = mpl.cm.seismic(np.linspace(0., 1., len(lvl_grad)))\n",
    "#_ = run_evaluation_spatial(score_engine, \"grad_amplitude\", \"1\", os.path.join(plot_dir, \"grad_amplitude_spatial\"), cmap=cmap_grad, levels=lvl_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel_juwels",
   "language": "python",
   "name": "langguth1_downscaling_kernel_juwels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
