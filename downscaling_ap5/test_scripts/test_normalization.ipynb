{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc16f8e2-30c8-4903-b4fc-ed12c175e21e",
   "metadata": {},
   "source": [
    "# Implementation of normalization class\n",
    "\n",
    "### Background\n",
    "To improve data normalization in MAELSTROM, a dedicated class for this job is introduced. <br>\n",
    "However, the implementation reveals some low-level bugs with xarray that are documented here.\n",
    "\n",
    "### Problem statement\n",
    "Let's start with the *errornous* approach in which we allow handling of datasets in the class. Note that this approach should work according to [xarray's documentation](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.std.html). It also works technically (i.e. it does not throw an error), but it produces strange results due to unrealistic values from the `std`-method on the dataset.\n",
    "\n",
    "The (verbose) source-code looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58144160-df72-478e-a5f4-68aa5e824354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, List\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "da_or_ds = Union[xr.DataArray, xr.Dataset]\n",
    "\n",
    "class Normalize(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for normalizing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: str, norm_dims: List):\n",
    "        self.method = method\n",
    "        self.norm_dims = norm_dims\n",
    "        self.norm_stats = None\n",
    "\n",
    "    def normalize(self, data: xr.DataArray, **stats):\n",
    "        \n",
    "        \n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_norm = self.normalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_norm\n",
    "\n",
    "    def denormalize(self, data: da_or_ds, **stats):\n",
    "        norm_stats = self.get_required_stats(data, *stats)\n",
    "        data_denorm = self.denormalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_denorm\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_stats(self, data, **stats):\n",
    "        \"\"\"\n",
    "        Function to retrieve either normalization parameters from data or from keyword arguments\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def normalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to normalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def denormalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to denormalize data.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d71c5a-f4b2-4659-a96b-2ec6a38860f4",
   "metadata": {},
   "source": [
    "The child class for z-score normalization looks then as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f575809e-72d0-4874-8de3-205ef6651275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScore(Normalize):\n",
    "\n",
    "    def __init__(self, norm_dims: List):\n",
    "        super().__init__(\"z_score\", norm_dims)\n",
    "        self.norm_stats = {\"mu\": None, \"sigma\": None}\n",
    "\n",
    "    def get_required_stats(self, data: da_or_ds, **stats):\n",
    "\n",
    "        mu, std = stats.get(\"mu\", self.norm_stats[\"mu\"]), stats.get(\"sigma\", self.norm_stats[\"sigma\"])\n",
    "\n",
    "        if mu is None or std is None:\n",
    "            print(\"Retrieve mu and sigma from data...\")\n",
    "            mu, std = data.mean(self.norm_dims), data.std(self.norm_dims)\n",
    "            self.norm_stats = {\"mu\": mu, \"sigma\": std}\n",
    "            print(self.norm_stats)\n",
    "        else:\n",
    "            print(\"Mu and sigma are parsed for (de-)normalization.\")\n",
    "            print(mu)\n",
    "            print(std)\n",
    "            \n",
    "        return mu, std\n",
    "            \n",
    "    @staticmethod     \n",
    "    def normalize_data(data, mu, std):\n",
    "        data_norm = (data - mu) / std\n",
    "        \n",
    "        return data_norm\n",
    "    \n",
    "    @staticmethod     \n",
    "    def denormalize_data(data, mu, std):\n",
    "        data_denorm = data * std + mu\n",
    "        \n",
    "        return data_denorm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94372889-2588-4f74-b60a-f361ed965948",
   "metadata": {},
   "source": [
    "We load the validation data of the downscaling Tier-2 dataset for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd3c886-7c74-45a6-81ce-24ee9f0070da",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/p/project/deepacf/maelstrom/data/ap5/tier2\"\n",
    "datafile = os.path.join(datadir, \"maelstrom-downscaling-tier2_test.nc\")\n",
    "\n",
    "ds = xr.open_dataset(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d347e22-1163-49fd-a88c-805a1f9e85c2",
   "metadata": {},
   "source": [
    "Then we apply the z-score normalization on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c90b057f-446b-435a-84d8-0fe1c60d18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve mu and sigma from data...\n",
      "{'mu': <xarray.Dataset>\n",
      "Dimensions:       ()\n",
      "Data variables:\n",
      "    rotated_pole  float64 1.0\n",
      "    2t_in         float32 282.8\n",
      "    sshf_in       float32 -7.078e+04\n",
      "    slhf_in       float32 -1.565e+05\n",
      "    blh_in        float32 529.6\n",
      "    10u_in        float32 0.1829\n",
      "    10v_in        float32 0.1448\n",
      "    z_in          float32 5.686e+03\n",
      "    t850_in       float32 278.6\n",
      "    t925_in       float32 282.7\n",
      "    hsurf_tar     float32 571.6\n",
      "    t_2m_tar      float32 283.2, 'sigma': <xarray.Dataset>\n",
      "Dimensions:       ()\n",
      "Data variables:\n",
      "    rotated_pole  float64 0.0\n",
      "    2t_in         float64 104.5\n",
      "    sshf_in       float64 1.973e+05\n",
      "    slhf_in       float64 2.177e+05\n",
      "    blh_in        float64 486.9\n",
      "    10u_in        float64 2.089\n",
      "    10v_in        float64 1.616\n",
      "    z_in          float64 4.809e+03\n",
      "    t850_in       float64 104.5\n",
      "    t925_in       float64 104.5\n",
      "    hsurf_tar     float64 511.0\n",
      "    t_2m_tar      float64 104.5}\n"
     ]
    }
   ],
   "source": [
    "norm_dims = [\"time\", \"rlat\", \"rlon\"]\n",
    "zscore_norm = ZScore(norm_dims)\n",
    "\n",
    "ds_norm1 = zscore_norm.normalize(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31860c01-4a87-4076-ba6a-d7aed8beb378",
   "metadata": {},
   "source": [
    "As we can see, the calculated standard deviation of the temperature variables (and others?) yields unexpected results which in would result into incorrect normalization. To prove this statement, we calculate the standard deviation manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81caad7d-55a4-4264-ae57-223ff34d7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray '2t_in' ()>\n",
      "array(9.052816, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t2m = ds[\"2t_in\"]\n",
    "sigma_t2m = np.sqrt(((t2m - t2m.mean(dim=norm_dims))**2).mean(dim=norm_dims))\n",
    "print(sigma_t2m)              # note that omitting the parameter-setting dim=norm_dims would give the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75227361-756f-44ff-bbb2-e43586f78646",
   "metadata": {},
   "source": [
    "By contrast, conversion of the dataset to a xr.DataArray, where the variables are assigned to a new dimension, gives the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32723cda-762e-4cad-9115-d11c13a96b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve mu and sigma from data...\n",
      "{'mu': <xarray.DataArray (variables: 12)>\n",
      "array([ 1.00000000e+00,  2.82832158e+02, -7.07827281e+04, -1.56503764e+05,\n",
      "        5.29617935e+02,  1.82885170e-01,  1.44832186e-01,  5.68644335e+03,\n",
      "        2.78623048e+02,  2.82670052e+02,  5.71572380e+02,  2.83245770e+02])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar', 'sigma': <xarray.DataArray (variables: 12)>\n",
      "array([0.00000000e+00, 9.05282455e+00, 2.24066973e+05, 2.36301865e+05,\n",
      "       5.11373825e+02, 2.43462441e+00, 1.87755217e+00, 4.48457913e+03,\n",
      "       7.45010479e+00, 8.21746109e+00, 4.97467674e+02, 9.03267566e+00])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar'}\n"
     ]
    }
   ],
   "source": [
    "da = ds.to_array(dim=\"variables\")\n",
    "\n",
    "zscore_norm2 = ZScore(norm_dims)     # get fresh class instance to avoid precomputed normalization parameters\n",
    "da_norm2 = zscore_norm2.normalize(da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcb809-7ce7-4cf4-8f50-71564ea6fa94",
   "metadata": {},
   "source": [
    "Let's perform some further tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90de7b07-82b4-416a-b196-818880060274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu and sigma are parsed for (de-)normalization.\n",
      "282.8321579222078\n",
      "9.052824545594893\n",
      "Retrieve mu and sigma from data...\n",
      "{'mu': <xarray.DataArray '2t_in' ()>\n",
      "array(282.83246, dtype=float32), 'sigma': <xarray.DataArray '2t_in' ()>\n",
      "array(9.052816, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "vardict = {\"variables\": \"2t_in\"}\n",
    "t2m_norm1 = da_norm2.sel(vardict)\n",
    "mu, std = zscore_norm2.norm_stats[\"mu\"].sel(vardict), zscore_norm2.norm_stats[\"sigma\"].sel(vardict)\n",
    "\n",
    "# normalize 2t_in with precomputed normalization parameters\n",
    "t2m_norm2 = zscore_norm2.normalize(t2m, mu=mu.values, sigma=std.values)\n",
    "# reset norm_stats manually...\n",
    "zscore_norm2.norm_stats = {\"mu\": None, \"sigma\": None}\n",
    "# ... to trigger computation\n",
    "t2m_norm3 = zscore_norm2.normalize(t2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bfb410",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ds_norm1 and t2m_norm2 differ!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(t2m_norm1, t2m_norm2, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-04\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt2m_norm1 and t2m_norm2 differ!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(t2m_norm2, t2m_norm3, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-04\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt2m_norm2 and t2m_norm3 differ!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(np\u001b[38;5;241m.\u001b[39misclose(ds_norm1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2t_in\u001b[39m\u001b[38;5;124m\"\u001b[39m], t2m_norm2, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.e-04\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds_norm1 and t2m_norm2 differ!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ds_norm1 and t2m_norm2 differ!"
     ]
    }
   ],
   "source": [
    "assert np.all(np.isclose(t2m_norm1, t2m_norm2, atol=1.e-04)), \"t2m_norm1 and t2m_norm2 differ!\"\n",
    "assert np.all(np.isclose(t2m_norm2, t2m_norm3, atol=1.e-04)), \"t2m_norm2 and t2m_norm3 differ!\"\n",
    "assert np.all(np.isclose(ds_norm1[\"2t_in\"], t2m_norm2, atol=1.e-04)), \"ds_norm1 and t2m_norm2 differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c7a1e-cd46-4d2f-996a-46028c261f5f",
   "metadata": {},
   "source": [
    "As expected, the normalized values differ substanially due to the errornous value for the standard deviataion.\n",
    "We can further trace the error which even happens when we apply the `std`-method on a Data Array with default-behaviur (i.e. without setting `dim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0ee471-c521-442d-9b8b-8a9a5c65a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'rlat', 'rlon']\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2018-01-01T01:00:00 ... 2018-12-31T23:00:00\n",
      "  * rlon     (rlon) float64 -8.273 -8.218 -8.163 -8.108 ... -1.838 -1.783 -1.728\n",
      "  * rlat     (rlat) float64 -3.933 -3.878 -3.823 -3.768 ... 1.182 1.237 1.292\n"
     ]
    }
   ],
   "source": [
    "print(norm_dims)\n",
    "print(ds[\"2t_in\"].coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25598bae-cad9-4a3e-be27-6791e11806c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:       ()\n",
      "Data variables:\n",
      "    rotated_pole  float64 0.0\n",
      "    2t_in         float64 104.5\n",
      "    sshf_in       float64 1.973e+05\n",
      "    slhf_in       float64 2.177e+05\n",
      "    blh_in        float64 486.9\n",
      "    10u_in        float64 2.089\n",
      "    10v_in        float64 1.616\n",
      "    z_in          float64 4.809e+03\n",
      "    t850_in       float64 104.5\n",
      "    t925_in       float64 104.5\n",
      "    hsurf_tar     float64 511.0\n",
      "    t_2m_tar      float64 104.5\n",
      "<xarray.DataArray '2t_in' ()>\n",
      "array(104.4526062)\n",
      "<xarray.DataArray '2t_in' ()>\n",
      "array(9.052816, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ds.std(skipna=True))                     # as above\n",
    "print(ds[\"2t_in\"].std(skipna=True))            # same error when retrievin 2t_in from the dataset only and defaulting .std()\n",
    "print(ds[\"2t_in\"].std(norm_dims, skipna=True))   # setting dimensions for .std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a593476-26ae-456e-a403-1cd55b7792ac",
   "metadata": {},
   "source": [
    "### The workaround\n",
    "\n",
    "In the following, we ensure that the normalization class does not work on `xr.Datasets`. Note furthermore, that `norm_dims=None` also fails since the default approach may run into the same issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca6c03ef-708a-4354-a817-057973f8f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "class Normalize(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for normalizing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: str, norm_dims: List):\n",
    "        self.method = method\n",
    "        self.norm_dims = norm_dims\n",
    "        self.norm_stats = None\n",
    "\n",
    "    def normalize(self, data: xr.DataArray, **stats):\n",
    "        \"\"\"\n",
    "        Normalize data.\n",
    "        :param data: The DataArray to be normalized.\n",
    "        :param **stats: Parameters to perform normalization. Must fit to normalization type!\n",
    "        :return: DataArray with normalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        if not isinstance(data, xr.DataArray):\n",
    "            raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "            \n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation            \n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_norm = self.normalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_norm\n",
    "\n",
    "    def denormalize(self, data: da_or_ds, **stats):\n",
    "        \"\"\"\n",
    "        Denormalize data.\n",
    "        :param data: The DataArray to be denormalized.\n",
    "        :param **stats: Parameters to perform denormalization. Must fit to normalization type!\n",
    "        :return: DataArray with denormalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        if not isinstance(data, xr.DataArray):\n",
    "            raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "            \n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation       \n",
    "        norm_stats = self.get_required_stats(data, *stats)\n",
    "        data_denorm = self.denormalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_denorm\n",
    "    \n",
    "    @property\n",
    "    def norm_dims(self):\n",
    "        return self._norm_dims\n",
    "    \n",
    "    @norm_dims.setter\n",
    "    def norm_dims(self, norm_dims):\n",
    "        if norm_dims is None:\n",
    "            raise AttributeError(\"norm_dims must not be None. Please parse a list of dimensions\" +\n",
    "                                 \"over which normalization should be applied.\")\n",
    "        \n",
    "        self._norm_dims = list(norm_dims)\n",
    "        \n",
    "    def _check_norm_dims(self, data):\n",
    "        \"\"\"\n",
    "        Check if dimension for normalization reside in dimensions of data.\n",
    "        :param data: the data (xr.DataArray) to be normalized\n",
    "        :return True: in case of passed check, a ValueError is risen else\n",
    "        \"\"\"\n",
    "        data_dims = list(data.dims)\n",
    "        norm_dims_check = [norm_dim in data_dims for norm_dim in self.norm_dims]\n",
    "        if not all(norm_dims_check):\n",
    "            imiss = np.where(~np.array(norm_dims_check))[0]\n",
    "            miss_dims = list(np.array(self.norm_dims)[imiss])\n",
    "            raise ValueError(\"The following dimensions do not reside in the data: \" +\n",
    "                             f\"{', '.join(miss_dims)}\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def save_norm_to_file(self, js_file):\n",
    "        \"\"\"\n",
    "        Write normalization parameters to file.\n",
    "        :param js_file: Path to JSON-file to be created.\n",
    "        :return: -\n",
    "        \"\"\"\n",
    "        if self.norm_stats is None:\n",
    "            raise AttributeError(\"norm_stats is still None. Please run (de-)normalization to get parameters.\")\n",
    "        \n",
    "        if any([stat is None for stat in self.norm_stats.values()]):\n",
    "            raise AttributeError(\"Some parameters of norm_stats are None.\")\n",
    "            \n",
    "        norm_serialized = {key: da.to_dict() for key, da in norm_dict.items()}\n",
    "        \n",
    "        with open(js_file, \"w\") as jsf:\n",
    "            js.dump(norm_dict_serialized, jsf)\n",
    "        \n",
    "    def read_norm_from_file(self, js_file):\n",
    "        \"\"\"\n",
    "        Read normalization parameters from file. Inverse function to write_norm_from_file.\n",
    "        :param js_file: Path to JSON-file to be read.\n",
    "        :return: Parameters set to self.norm_stats\n",
    "        \"\"\"\n",
    "        with open(js_file, \"r\") as jsf:\n",
    "            norm_data = js.load(jsf)\n",
    "            \n",
    "        norm_dict_restored = {key: xr.DataArray.from_dict(da_dict) for key, da_dict in norm_data.items()}\n",
    "        \n",
    "        self.norm_stats = norm_dict_restored    \n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_stats(self, data, **stats):\n",
    "        \"\"\"\n",
    "        Function to retrieve either normalization parameters from data or from keyword arguments\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def normalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to normalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def denormalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to denormalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class ZScore(Normalize):\n",
    "\n",
    "    def __init__(self, norm_dims: List):\n",
    "        super().__init__(\"z_score\", norm_dims)\n",
    "        self.norm_stats = {\"mu\": None, \"sigma\": None}\n",
    "\n",
    "    def get_required_stats(self, data: da_or_ds, **stats):\n",
    "        \"\"\"\n",
    "        Get required parameters for z-score normalization. They are either computed from the data \n",
    "        or can be parsed as keyword arguments.\n",
    "        :param data: the data to be (de-)normalized\n",
    "        :param mu: keyword argument for mean used for normalization\n",
    "        :param sigma: keyword argument for standard deviation for normalization\n",
    "        :return (mu, sigma): Parameters for normalization\n",
    "        \"\"\"\n",
    "        mu, std = stats.get(\"mu\", self.norm_stats[\"mu\"]), stats.get(\"sigma\", self.norm_stats[\"sigma\"])\n",
    "\n",
    "        if mu is None or std is None:\n",
    "            print(\"Retrieve mu and sigma from data...\")\n",
    "            mu, std = data.mean(self.norm_dims), data.std(self.norm_dims)\n",
    "            self.norm_stats = {\"mu\": mu, \"sigma\": std}\n",
    "            print(self.norm_stats)\n",
    "        else:\n",
    "            print(\"Mu and sigma are parsed for (de-)normalization.\")\n",
    "            print(mu)\n",
    "            print(std)\n",
    "            \n",
    "        return mu, std\n",
    "            \n",
    "    @staticmethod     \n",
    "    def normalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score normalization on data\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for normalization\n",
    "        :param std: standard deviation of data for normalization\n",
    "        :return data_norm: normalized data\n",
    "        \"\"\"\n",
    "        data_norm = (data - mu) / std\n",
    "        \n",
    "        return data_norm\n",
    "    \n",
    "    @staticmethod     \n",
    "    def denormalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score denormalization on data.\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for denormalization\n",
    "        :param std: standard deviation of data for denormalization\n",
    "        :return data_norm: denormalized data\n",
    "        \"\"\"\n",
    "        data_denorm = data * std + mu\n",
    "        \n",
    "        return data_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbeecf-1472-4eb0-b679-e8c5e530fe53",
   "metadata": {},
   "source": [
    "To enable data processing, we start by converting the dataset to a data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b105ab1-7f94-43cf-b9c9-694cb78ba47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds.to_array(dim=\"variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4277d55-3b4a-4a06-9453-f208aff78323",
   "metadata": {},
   "source": [
    "Let's perform various tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3862e034-632d-4969-a729-f0baa8407641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve mu and sigma from data...\n",
      "{'mu': <xarray.DataArray (variables: 12)>\n",
      "array([ 1.00000000e+00,  2.82832158e+02, -7.07827281e+04, -1.56503764e+05,\n",
      "        5.29617935e+02,  1.82885170e-01,  1.44832186e-01,  5.68644335e+03,\n",
      "        2.78623048e+02,  2.82670052e+02,  5.71572380e+02,  2.83245770e+02])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar', 'sigma': <xarray.DataArray (variables: 12)>\n",
      "array([0.00000000e+00, 9.05282455e+00, 2.24066973e+05, 2.36301865e+05,\n",
      "       5.11373825e+02, 2.43462441e+00, 1.87755217e+00, 4.48457913e+03,\n",
      "       7.45010479e+00, 8.21746109e+00, 4.97467674e+02, 9.03267566e+00])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar'}\n"
     ]
    }
   ],
   "source": [
    "zscore_norm = ZScore([\"rlat\", \"rlon\", \"time\"])\n",
    "\n",
    "# normalize the resulting Data Array...\n",
    "da_norm = zscore_norm.normalize(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3dd17071-3c1d-4c88-90c2-f624d17b6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and compare:\n",
    "assert np.all(np.isclose(da_norm.sel({\"variables\": \"2t_in\"}), t2m_norm1, atol=1.e-06)), \"da_norm and t2m_norm1 differ!\"\n",
    "assert np.all(np.isclose(t2m_norm1, t2m_norm2, atol=1.e-04)), \"t2m_norm1 and t2m_norm2 differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c6293-465d-4dac-b9ef-41fb655bc162",
   "metadata": {},
   "source": [
    "Except for spurious deviations due to changes in the normalization parameters (why ever this happens?!), the results now coincide. At least, errors smaller than 1.e-04 can also be neglected in the normalized space for our purposes which aims to project the values into a data range suitable for backpropgation in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed27c26-b83a-4c1b-a26a-c5ebbdc7321f",
   "metadata": {},
   "source": [
    "In the following, we perform some further tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c096846d-798b-48fe-9a98-c23477ed4d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu and sigma are parsed for (de-)normalization.\n",
      "<xarray.DataArray (variables: 12)>\n",
      "array([ 1.00000000e+00,  2.82832158e+02, -7.07827281e+04, -1.56503764e+05,\n",
      "        5.29617935e+02,  1.82885170e-01,  1.44832186e-01,  5.68644335e+03,\n",
      "        2.78623048e+02,  2.82670052e+02,  5.71572380e+02,  2.83245770e+02])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar'\n",
      "<xarray.DataArray (variables: 12)>\n",
      "array([0.00000000e+00, 9.05282455e+00, 2.24066973e+05, 2.36301865e+05,\n",
      "       5.11373825e+02, 2.43462441e+00, 1.87755217e+00, 4.48457913e+03,\n",
      "       7.45010479e+00, 8.21746109e+00, 4.97467674e+02, 9.03267566e+00])\n",
      "Coordinates:\n",
      "  * variables  (variables) <U12 'rotated_pole' '2t_in' ... 't_2m_tar'\n"
     ]
    }
   ],
   "source": [
    "js_file = \"./test.json\"\n",
    "\n",
    "# save normalization parameters to file\n",
    "zscore_norm.save_norm_to_file(js_file)\n",
    "\n",
    "# instantiate fresh instance and get normalization parameters from file\n",
    "zscore_norm_new = ZScore([\"rlat\", \"rlon\", \"time\"])\n",
    "zscore_norm_new.read_norm_from_file(js_file)\n",
    "\n",
    "# apply normalization without retrieval from data (see print-statement!)\n",
    "da_norm = zscore_norm_new.normalize(da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6cd6c-80ff-4bac-b4e1-fa02e40ac4af",
   "metadata": {},
   "source": [
    "Finally check, if we still obtain the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0059570f-e5c8-41e5-8992-2eeb3786e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.isclose(da_norm.sel({\"variables\": \"2t_in\"}), t2m_norm1, atol=1.e-06)), \"da_norm and t2m_norm1 differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c33a3-460c-484f-b007-2dfb96a1cd4c",
   "metadata": {},
   "source": [
    "## 2023-04-04: Further tests \n",
    "\n",
    "### Problem statement\n",
    "When retrieving the normalization parameters from xarray `Datasets` which then gets applied to xarray `DataArrays`, unwanted conversion of the data object or even a failure can be observed. Thus, the code below will further investigate how to ensure proper 'cross-application' of the (de-)normalization procedure. <br><br>\n",
    "The current base class as obtained from branch #067 (commit ???):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d377e3-af04-4aa1-99f1-9c053e54c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "class Normalize(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for normalizing data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method: str, norm_dims: List):\n",
    "        self.method = method\n",
    "        self.norm_dims = norm_dims\n",
    "        self.norm_stats = None\n",
    "\n",
    "    def normalize(self, data: xr.DataArray, **stats):\n",
    "        \"\"\"\n",
    "        Normalize data.\n",
    "        :param data: The DataArray to be normalized.\n",
    "        :param **stats: Parameters to perform normalization. Must fit to normalization type!\n",
    "        :return: DataArray with normalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        # if not isinstance(data, xr.DataArray):\n",
    "        #    raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "\n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation\n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_norm = self.normalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_norm\n",
    "\n",
    "    def denormalize(self, data: da_or_ds, **stats):\n",
    "        \"\"\"\n",
    "        Denormalize data.\n",
    "        :param data: The DataArray to be denormalized.\n",
    "        :param **stats: Parameters to perform denormalization. Must fit to normalization type!\n",
    "        :return: DataArray with denormalized data.\n",
    "        \"\"\"\n",
    "        # sanity checks\n",
    "        # if not isinstance(data, xr.DataArray):\n",
    "        #    raise TypeError(f\"Passed data must be a xarray.DataArray, but is of type {str(type(data))}.\")\n",
    "\n",
    "        _ = self._check_norm_dims(data)\n",
    "        # do the computation\n",
    "        norm_stats = self.get_required_stats(data, **stats)\n",
    "        data_denorm = self.denormalize_data(data, *norm_stats)\n",
    "\n",
    "        return data_denorm\n",
    "\n",
    "    @property\n",
    "    def norm_dims(self):\n",
    "        return self._norm_dims\n",
    "\n",
    "    @norm_dims.setter\n",
    "    def norm_dims(self, norm_dims):\n",
    "        if norm_dims is None:\n",
    "            raise AttributeError(\"norm_dims must not be None. Please parse a list of dimensions\" +\n",
    "                                 \"over which normalization should be applied.\")\n",
    "\n",
    "        self._norm_dims = list(norm_dims)\n",
    "\n",
    "    def _check_norm_dims(self, data):\n",
    "        \"\"\"\n",
    "        Check if dimension for normalization reside in dimensions of data.\n",
    "        :param data: the data (xr.DataArray) to be normalized\n",
    "        :return True: in case of passed check, a ValueError is risen else\n",
    "        \"\"\"\n",
    "        data_dims = list(data.dims)\n",
    "        norm_dims_check = [norm_dim in data_dims for norm_dim in self.norm_dims]\n",
    "        if not all(norm_dims_check):\n",
    "            imiss = np.where(~np.array(norm_dims_check))[0]\n",
    "            miss_dims = list(np.array(self.norm_dims)[imiss])\n",
    "            raise ValueError(\"The following dimensions do not reside in the data: \" +\n",
    "                             f\"{', '.join(miss_dims)}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_norm_to_file(self, js_file, missdir_ok: bool = True):\n",
    "        \"\"\"\n",
    "        Write normalization parameters to file.\n",
    "        :param js_file: Path to JSON-file to be created.\n",
    "        :param missdir_ok: If True, base-directory of JSON-file can be missing and will be created then.\n",
    "        :return: -\n",
    "        \"\"\"\n",
    "        if self.norm_stats is None:\n",
    "            raise AttributeError(\"norm_stats is still None. Please run (de-)normalization to get parameters.\")\n",
    "\n",
    "        if any([stat is None for stat in self.norm_stats.values()]):\n",
    "            raise AttributeError(\"Some parameters of norm_stats are None.\")\n",
    "\n",
    "        norm_serialized = {key: da.to_dict() for key, da in self.norm_stats.items()}\n",
    "\n",
    "        # serialization and (later) deserialization depends on data type.\n",
    "        # Thus, we have to save it to the dictionary\n",
    "        d0 = list(self.norm_stats.values())[0]\n",
    "        if isinstance(d0, xr.DataArray):\n",
    "            norm_serialized[\"data_type\"] = \"data_array\"\n",
    "        elif isinstance(d0, xr.Dataset):\n",
    "            norm_serialized[\"data_type\"] = \"data_set\"\n",
    "\n",
    "        if missdir_ok: os.makedirs(os.path.dirname(js_file), exist_ok=True)\n",
    "\n",
    "        with open(js_file, \"w\") as jsf:\n",
    "            js.dump(norm_serialized, jsf)\n",
    "\n",
    "    def read_norm_from_file(self, js_file):\n",
    "        \"\"\"\n",
    "        Read normalization parameters from file. Inverse function to write_norm_from_file.\n",
    "        :param js_file: Path to JSON-file to be read.\n",
    "        :return: Parameters set to self.norm_stats\n",
    "        \"\"\"\n",
    "        with open(js_file, \"r\") as jsf:\n",
    "            norm_data = js.load(jsf)\n",
    "\n",
    "        data_type = norm_data.pop('data_type', None)\n",
    "\n",
    "        if data_type == \"data_array\":\n",
    "            xr_obj = xr.DataArray\n",
    "        elif data_type == \"data_set\":\n",
    "            xr_obj = xr.Dataset\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown data_type {data_type} in {js_file}. Only 'data_array' or 'data_set' are allowed.\")\n",
    "\n",
    "        norm_data.pop('data_type', None)\n",
    "\n",
    "        norm_dict_restored = {key: xr_obj.from_dict(da_dict) for key, da_dict in norm_data.items()}\n",
    "\n",
    "        self.norm_stats = norm_dict_restored\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_required_stats(self, data, *stats):\n",
    "        \"\"\"\n",
    "        Function to retrieve either normalization parameters from data or from keyword arguments\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def normalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to normalize data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def denormalize_data(data, *norm_param):\n",
    "        \"\"\"\n",
    "        Function to denormalize data.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ae269-dd2b-4be2-abe4-de0c7fa6ff07",
   "metadata": {},
   "source": [
    "The child class for zscore-normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf7dfc5d-d093-4fd5-94f3-6eae6a68b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScore(Normalize):\n",
    "    def __init__(self, norm_dims: List):\n",
    "        super().__init__(\"z_score\", norm_dims)\n",
    "        self.norm_stats = {\"mu\": None, \"sigma\": None}\n",
    "\n",
    "    def get_required_stats(self, data: da_or_ds, **stats):\n",
    "        \"\"\"\n",
    "        Get required parameters for z-score normalization. They are either computed from the data\n",
    "        or can be parsed as keyword arguments.\n",
    "        :param data: the data to be (de-)normalized\n",
    "        :param stats: keyword arguments for mean (mu) and standard deviation (std) used for normalization\n",
    "        :return (mu, sigma): Parameters for normalization\n",
    "        \"\"\"\n",
    "        mu, std = stats.get(\"mu\", self.norm_stats[\"mu\"]), stats.get(\"sigma\", self.norm_stats[\"sigma\"])\n",
    "\n",
    "        if mu is None or std is None:\n",
    "            print(\"Retrieve mu and sigma from data...\")\n",
    "            mu, std = data.mean(self.norm_dims), data.std(self.norm_dims)\n",
    "            # the following ensure that both parameters are computed in one graph!\n",
    "            # This significantly reduces memory footprint as we don't end up having data duplicates\n",
    "            # in memory due to multiple graphs (and also seem to enfore usage of data chunks as well)\n",
    "            mu, std = dask.compute(mu, std)\n",
    "            self.norm_stats = {\"mu\": mu, \"sigma\": std}\n",
    "        # else:\n",
    "        #    print(\"Mu and sigma are parsed for (de-)normalization.\")\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score normalization on data\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for normalization\n",
    "        :param std: standard deviation of data for normalization\n",
    "        :return data_norm: normalized data\n",
    "        \"\"\"\n",
    "        data = (data - mu) / std\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalize_data(data, mu, std):\n",
    "        \"\"\"\n",
    "        Perform z-score denormalization on data.\n",
    "        :param data: Data array of interest\n",
    "        :param mu: mean of data for denormalization\n",
    "        :param std: standard deviation of data for denormalization\n",
    "        :return data_norm: denormalized data\n",
    "        \"\"\"\n",
    "        data = data * std + mu\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b93d3fc-4e50-4573-bc0e-d347468f4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_ds(ds):\n",
    "    \"\"\"\n",
    "    Convert a xarray dataset to a data-array where the variables will constitute the last dimension (channel last)\n",
    "    :param ds: the xarray dataset with dimensions (dims)\n",
    "    :return da: the data-array with dimensions (dims, variables)\n",
    "    \"\"\"\n",
    "    da = ds.to_array(dim=\"variables\")\n",
    "    print(da)\n",
    "    da = da.transpose(..., \"variables\")\n",
    "    return da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876d678-dba5-46e0-ab66-7bfcac0806e6",
   "metadata": {},
   "source": [
    "Next, we set the path to some test data (namely the augmented Tier-1 dataset)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e85e3a75-2465-48df-8638-45ae2305f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier1/netcdf_data/workdir/\"\n",
    "\n",
    "fname_train, fname_val = os.path.join(datadir, \"downscaling_tier1_train_aug.nc\"), os.path.join(datadir, \"downscaling_tier1_val_aug.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c757c-fd61-4897-9bab-6ed9b32cb5ff",
   "metadata": {},
   "source": [
    "... and load the training data while also calculating the normalization parameters with the `xr.DataArray` after applying `reshape_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "210cc8ef-807d-4610-9b55-f7adf516e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading the trining data from file '/p/scratch/deepacf/maelstrom/maelstrom_data/ap5_michael/preprocessed_tier1/netcdf_data/workdir/downscaling_tier1_train_aug.nc'...\n",
      "<xarray.DataArray (variables: 4, time: 20496, lat: 96, lon: 128)>\n",
      "array([[[[ 2.7890393e+02,  2.7890341e+02,  2.7890289e+02, ...,\n",
      "           2.7840961e+02,  2.7842529e+02,  2.7844098e+02],\n",
      "         [ 2.7893362e+02,  2.7893417e+02,  2.7893475e+02, ...,\n",
      "           2.7864780e+02,  2.7866364e+02,  2.7867950e+02],\n",
      "         [ 2.7896332e+02,  2.7896497e+02,  2.7896661e+02, ...,\n",
      "           2.7888596e+02,  2.7890198e+02,  2.7891803e+02],\n",
      "         ...,\n",
      "         [ 2.7672256e+02,  2.7678378e+02,  2.7684503e+02, ...,\n",
      "           2.9391086e+02,  2.9382962e+02,  2.9374838e+02],\n",
      "         [ 2.7706863e+02,  2.7717160e+02,  2.7727454e+02, ...,\n",
      "           2.9351395e+02,  2.9345609e+02,  2.9339825e+02],\n",
      "         [ 2.7741473e+02,  2.7755939e+02,  2.7770404e+02, ...,\n",
      "           2.9311707e+02,  2.9308258e+02,  2.9304810e+02]],\n",
      "\n",
      "        [[ 2.7905771e+02,  2.7905225e+02,  2.7904678e+02, ...,\n",
      "           2.7872281e+02,  2.7871896e+02,  2.7871509e+02],\n",
      "         [ 2.7908643e+02,  2.7908072e+02,  2.7907504e+02, ...,\n",
      "           2.7897202e+02,  2.7896890e+02,  2.7896579e+02],\n",
      "         [ 2.7911514e+02,  2.7910922e+02,  2.7910333e+02, ...,\n",
      "           2.7922122e+02,  2.7921884e+02,  2.7921646e+02],\n",
      "...\n",
      "           2.8667957e+02,  2.8665753e+02,  2.8664755e+02],\n",
      "         [ 2.8632825e+02,  2.8646478e+02,  2.8668738e+02, ...,\n",
      "           2.8673932e+02,  2.8669235e+02,  2.8666391e+02],\n",
      "         [ 2.8673505e+02,  2.8690359e+02,  2.8718521e+02, ...,\n",
      "           2.8671371e+02,  2.8666678e+02,  2.8664188e+02]],\n",
      "\n",
      "        [[ 2.9534943e+02,  2.9482883e+02,  2.9496609e+02, ...,\n",
      "           2.8979803e+02,  2.9095227e+02,  2.9218826e+02],\n",
      "         [ 2.9460767e+02,  2.9481534e+02,  2.9484662e+02, ...,\n",
      "           2.9074530e+02,  2.9236038e+02,  2.9352457e+02],\n",
      "         [ 2.9479114e+02,  2.9460623e+02,  2.9433957e+02, ...,\n",
      "           2.9238669e+02,  2.9369595e+02,  2.9394272e+02],\n",
      "         ...,\n",
      "         [ 2.8625003e+02,  2.8638870e+02,  2.8656650e+02, ...,\n",
      "           2.8658713e+02,  2.8656650e+02,  2.8655298e+02],\n",
      "         [ 2.8648611e+02,  2.8663690e+02,  2.8687015e+02, ...,\n",
      "           2.8669449e+02,  2.8665610e+02,  2.8662408e+02],\n",
      "         [ 2.8691708e+02,  2.8710910e+02,  2.8740710e+02, ...,\n",
      "           2.8675851e+02,  2.8671371e+02,  2.8668170e+02]]]],\n",
      "      dtype=float32)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 2016-04-01T10:00:00 ... 2019-09-30T16:03:00\n",
      "  * lon        (lon) float64 4.0 4.1 4.2 4.3 4.4 ... 16.3 16.4 16.5 16.6 16.7\n",
      "  * lat        (lat) float64 54.5 54.4 54.3 54.2 54.1 ... 45.3 45.2 45.1 45.0\n",
      "  * variables  (variables) <U7 't2m_in' 'z_in' 'z_tar' 't2m_tar'\n",
      "Attributes:\n",
      "    CDI:                        Climate Data Interface version 2.0.2 (https:/...\n",
      "    Conventions:                CF-1.6\n",
      "    history:                    Sun Mar 06 21:57:45 2022: cdo mergetime 2016/...\n",
      "    NCO:                        netCDF Operators version 4.9.5 (Homepage = ht...\n",
      "    history_of_appended_files:  Sun Mar  6 21:09:55 2022: Appended file /p/sc...\n",
      "    CDO:                        Climate Data Operators version 2.0.2 (https:/...\n",
      "Retrieve mu and sigma from data...\n"
     ]
    }
   ],
   "source": [
    "data_norm = ZScore([\"time\", \"lat\", \"lon\"])\n",
    "\n",
    "print(f\"Start loading the trining data from file '{fname_train}'...\")\n",
    "ds_train = xr.open_dataset(fname_train)\n",
    "da_train = reshape_ds(ds_train.astype(\"float32\", copy=False))\n",
    "da_train = data_norm.normalize(da_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a7d2-ac10-4b58-bc5e-2827d7274c90",
   "metadata": {},
   "source": [
    "Next, we load the validation data, but keep as a `xr.Dataset`. Noe that we have retrieved the normalization parameters from a `xr.DataArray`-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7ce769e4-75c1-439f-80dd-f48be040069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 2604, lon: 128, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2020-05-01T10:00:00 ... 2020-08-31T16:03:00\n",
      "  * lon      (lon) float64 4.0 4.1 4.2 4.3 4.4 4.5 ... 16.3 16.4 16.5 16.6 16.7\n",
      "  * lat      (lat) float64 54.5 54.4 54.3 54.2 54.1 ... 45.4 45.3 45.2 45.1 45.0\n",
      "Data variables:\n",
      "    t2m_in   (time, lat, lon) float64 ...\n",
      "    z_in     (time, lat, lon) float64 ...\n",
      "    z_tar    (time, lat, lon) float64 ...\n",
      "    t2m_tar  (time, lat, lon) float64 ...\n",
      "Attributes:\n",
      "    CDI:                        Climate Data Interface version 2.0.2 (https:/...\n",
      "    Conventions:                CF-1.6\n",
      "    history:                    Sun Mar 06 21:57:45 2022: cdo mergetime 2016/...\n",
      "    NCO:                        netCDF Operators version 4.9.5 (Homepage = ht...\n",
      "    history_of_appended_files:  Sun Mar  6 21:09:55 2022: Appended file /p/sc...\n",
      "    CDO:                        Climate Data Operators version 2.0.2 (https:/...\n"
     ]
    }
   ],
   "source": [
    "ds_val = xr.open_dataset(fname_val)\n",
    "print(ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4061826-1118-4c3c-b68e-16c3f4f0cbbd",
   "metadata": {},
   "source": [
    "To allow flexible corecing between different `xarray` data types, we introduce the following auxiliary function which performs this job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51713b8e-f61b-457d-b0d9-102b1e64e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_datatype(data, *args, var_dim=\"variables\"):\n",
    "    \n",
    "    # sanity check \n",
    "    ds_or_da = (xr.Dataset, xr.DataArray)\n",
    "    all_args = [data] + list(args)\n",
    "    if not all(isinstance(arg, ds_or_da) for arg in all_args):\n",
    "        flags = [not isinstance(arg, ds_or_da) for arg in all_args]\n",
    "        inds = np.nonzero(flags)[0].tolist()#[0]\n",
    "        if len(inds) == 1:\n",
    "            err_str = f\"The parsed argument at position {inds} is\"\n",
    "        else:\n",
    "            err_str = f\"The parsed arguments at positions {inds} are\"\n",
    "        raise ValueError(f\"{err_str} not an xarray.DataArray or xarray.Dataset.\")\n",
    "    \n",
    "    # align type of arguments if required\n",
    "    if isinstance(data, type(args[0])):\n",
    "        args_new = args\n",
    "    elif isinstance(data, xr.Dataset) and isinstance(args[0], xr.DataArray):\n",
    "        args_new = tuple(arg.to_dataset(dim=var_dim) for arg in args)\n",
    "    elif isinstance(data, xr.DataArray) and isinstance(args[0], xr.Dataset):\n",
    "        args_new = tuple(arg.to_array(dim=var_dim) for arg in args)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown error occured. Please check all input parameters.\")\n",
    "        \n",
    "    return args_new\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770769dc-2e85-42cf-a95b-b60a3d4bff05",
   "metadata": {},
   "source": [
    "Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "45f9935b-724e-449e-86ea-2fb270af01f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 2604, lon: 128, lat: 96)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2020-05-01T10:00:00 ... 2020-08-31T16:03:00\n",
      "  * lon      (lon) float64 4.0 4.1 4.2 4.3 4.4 4.5 ... 16.3 16.4 16.5 16.6 16.7\n",
      "  * lat      (lat) float64 54.5 54.4 54.3 54.2 54.1 ... 45.4 45.3 45.2 45.1 45.0\n",
      "Data variables:\n",
      "    t2m_in   (time, lat, lon) float64 -1.783 -1.785 -1.787 ... -0.5563 -0.5673\n",
      "    z_in     (time, lat, lon) float64 -0.9632 -0.9632 ... -0.9632 -0.9632\n",
      "    z_tar    (time, lat, lon) float64 -0.8164 -0.8165 ... -0.8165 -0.8164\n",
      "    t2m_tar  (time, lat, lon) float64 -1.708 -1.713 -1.715 ... -0.4749 -0.4918\n"
     ]
    }
   ],
   "source": [
    "mu, std = match_datatype(data, data_norm.norm_stats[\"mu\"], data_norm.norm_stats[\"sigma\"])\n",
    "#mu, std = match_datatype(data, np.arange(4), np.arange(4))\n",
    "data = (ds_val - mu) / std\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel_juwels",
   "language": "python",
   "name": "langguth1_downscaling_kernel_juwels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
