{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-afternoon",
   "metadata": {},
   "source": [
    "# Benchmarking the application \"Downscaling of 2m temperature from IFS HRES with a U-Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install climetlab==0.8.14\n",
    "!pip install climetlab-maelstrom-downscaling==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.utils as ku\n",
    "sys.path += [\"../handle_data/\", \"../models\", \"../postprocess/\"]\n",
    "from handle_data_unet import *\n",
    "from unet_model import build_unet, get_lr_scheduler\n",
    "import xarray as xr\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/p/project/deepacf/maelstrom/data/downscaling_unet/\"\n",
    "\n",
    "data_obj = HandleUnetData(datadir, \"train\")\n",
    "data_obj.append_data(\"val\")\n",
    "data_obj.append_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set daytime for which downsclaing model is trained (i.e. either 0 or 12)\n",
    "hour = 12    \n",
    "\n",
    "# preprocess data for training\n",
    "int_data, tart_data, opt_norm = data_obj.normalize(\"train\", daytime=12)\n",
    "inv_data, tarv_data = data_obj.normalize(\"val\", daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "print(data_obj.timing)\n",
    "print(data_obj.data_info[\"memory_datasets\"])\n",
    "print(data_obj.data_info[\"nsamples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as ku\n",
    "shape_in = (96, 128, 3)\n",
    "\n",
    "if \"login\" in data_obj.host:\n",
    "    unet_model = build_unet(shape_in, z_branch=True)\n",
    "    ku.plot_model(unet_model, to_file=os.path.join(os.getcwd(), \"unet_downscaling_model.png\"), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class for creating timer callback\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "z_branch = True                    # flag if additionally training on surface elevation is performed\n",
    "\n",
    "lr_scheduler, time_tracker = get_lr_scheduler(), TimeHistory()\n",
    "# create callbacks\n",
    "callback_list = [lr_scheduler, time_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build, compile and train the model\n",
    "nepochs = 70\n",
    "unet_model = build_unet(shape_in, z_branch=z_branch)\n",
    "if z_branch:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)),\n",
    "                   loss={\"output_temp\": \"mae\", \"output_z\": \"mae\"}, \n",
    "                   loss_weights={\"output_temp\": 1.0, \"output_z\": 1.0})\n",
    "    \n",
    "    history = unet_model.fit(x=int_data.values, y={\"output_temp\": tart_data.isel(variable=0).values,\n",
    "                                                   \"output_z\": tart_data.isel(variable=1).values},\n",
    "                             batch_size=32, epochs=nepochs, callbacks=callback_list, \n",
    "                             validation_data=(inv_data.values, {\"output_temp\": tarv_data.isel(variable=0).values,\n",
    "                                                                \"output_z\": tarv_data.isel(variable=1).values}))\n",
    "else:\n",
    "    unet_model.compile(optimizer=Adam(learning_rate=5*10**(-4)), loss=\"mae\")\n",
    "\n",
    "    history = unet_model.fit(x=int_data.values, y=tart_data.isel(variable=0).values, batch_size=32,\n",
    "                             epochs=nepochs, callbacks=callback_list,\n",
    "                             validation_data=(inv_data.values, tarv_data.isel(variable=0).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_times = time_tracker.epoch_times\n",
    "\n",
    "print(history.history[\"output_temp_loss\"][-1])\n",
    "print(history.history[\"val_output_temp_loss\"][-1])\n",
    "\n",
    "print(\"Total training time: {0:.2f}s\".format(np.sum(epoch_times)))\n",
    "print(\"Max. time per epoch: {0:.4f}s, min. time per epoch: {1:.4f}s\".format(np.amax(epoch_times), np.amin(epoch_times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test data first\n",
    "inte_data, tarte_data = data_obj.preprocess_data(\"test\", daytime=hour, opt_norm=opt_norm)\n",
    "\n",
    "# generate the downscaled fields\n",
    "y_pred_test = unet_model.predict(inte_data.values, verbose=1)\n",
    "y_pred_val = unet_model.predict(inv_data.values, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_type = \"validation\"            # change here to switch between validation and testing data\n",
    "if comparison_type == \"validation\":\n",
    "  y_pred = y_pred_val\n",
    "  ds_ref = data_obj.data[\"val\"].sel(time=dt.time(hour))\n",
    "  var_ref = tarv_data.isel(variable=0)\n",
    "elif comparison_type == \"test\":\n",
    "  y_pred = y_pred_test\n",
    "  ds_ref = data_obj.data[\"test\"].sel(time=dt.time(hour))\n",
    "  var_ref = tarte_data.isel(variable=0)\n",
    "else:\n",
    "  ValueError(\"Unknown comparison_type '{0}' chosen.\".format(comparison_type))\n",
    "\n",
    "if np.ndim(y_pred) == 5:                # cropping necessary if z_branch is True (two output channels)\n",
    "  y_pred = y_pred[0]\n",
    "else:\n",
    "  pass\n",
    "\n",
    "print(np.abs(np.squeeze(y_pred) - var_ref).mean(dim=[\"lat\", \"lon\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some relevant information from the original dataset, ...\n",
    "coords = var_ref.squeeze().coords\n",
    "dims = var_ref.squeeze().dims\n",
    "\n",
    "# denomralize...\n",
    "y_pred_trans = np.squeeze(y_pred)*opt_norm[\"std_tar\"].squeeze().values + opt_norm[\"mu_tar\"].squeeze().values\n",
    "# and make xarray DataArray \n",
    "y_pred_trans = xr.DataArray(y_pred_trans, coords=coords, dims=dims, name=\"t2m_downscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-passing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = ((y_pred_trans - ds_ref[\"t2m_tar\"])**2).mean(dim=[\"lat\", \"lon\"])\n",
    "\n",
    "print(\"MSE of downscaled 2m temperature: {0:.3f} K**2 (+/-{1:.3f} K**2)\".format(mse.mean().values, mse.std().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-adoption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "understanding-audience",
   "metadata": {},
   "source": [
    "As we see, the model has learned to recover a lot of details resulting mainly from the topography. Especially over the Alpes, but also over the the German low mountain ranges, the differences have become smaller and less structured. It is also noted that the differences near the coast (e.g. at the Baltic Sea) have become smaller. <br>\n",
    "However, some systematic features are still visible, the differences can stilll be as large as 3 K and especially in the Alps, the differences are somehow 'blurry'. Thus, there is still room for further improvement. \n",
    "These improvements will not only pertain the model architecture, but will also target to engulf more meteorological variables. The latter will also enable the network to generalize with respect to daytime and season. Note, that this has not been done yet, since we trained the U-net with data between April and September at 12 UTC only.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.0",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
