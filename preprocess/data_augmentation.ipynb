{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7bcdd7-d2e9-4e5a-bee3-642857a4613f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data augmented dataset for 2m temperature downscaing with U-net\n",
    "\n",
    "So far, the dataset used for training the U-net in the first deliverable is fairly small (only ~300 MB of training data).\n",
    "However, for benchmarking the [HPC-systems]() a larger dataset is considered to be more reasonable.\n",
    "For the sake of the related deliverable, the dataset is therefore augmented as follows:\n",
    "- Add more daytimes (e.g. 10-16 UTC) instead of choosing on one daytime (e.g. 12 UTC)\n",
    "- Perform simple data augmentation by flipping along the geographical axis (latitude and longitude)\n",
    "\n",
    "In total, this increases the number of samples by a factor of 7x4=28. The complete dataset should the comprise 25.620 samples.\n",
    "\n",
    "As a prepartory step, the preprocessing with the Python-script `preprocess_downscaling_data.py` must be performed in which the original IFS HRES data is processed (with lead times between 0 and 11 hours).\n",
    "For this purpose, set-up `preprocess_ifs_hres_data_template.sh` accordingly and run the resulting runscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd30463-9f93-46ce-bdd9-e9d63aa2c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d303f-276b-48bd-97d6-0867eb865a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up data-directory and load all merged netCDf-files of complete dataset\n",
    "datadir = \"/p/scratch/deepacf/maelstrom/maelstrom_data/ifs_hres/preprocessed/netcdf_data/workdir\"\n",
    "data_all = xr.open_mfdataset(os.path.join(datadir, \"sfc_*_merged.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d818b-f33d-4795-9be0-c643bf371ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice data to daytimes of interest (i.e. between 10 and 17 UTC)\n",
    "daytimes = list(range(10, 17))\n",
    "\n",
    "data_all_sub = data_all.sel(time=data_all.time.dt.hour.isin(daytimes)).load()\n",
    "times= data_all_sub[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c90a4-0683-4690-bc05-698e8bd69ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation\n",
    "\n",
    "# generate data-array with flipped latitude axis\n",
    "data_all_sub_invlat = data_all_sub.reindex(lat=data_all_sub.lat[::-1])\n",
    "data_all_sub_invlat.coords[\"lat\"] = data_all_sub[\"lat\"]\n",
    "# generate data-array with flipped longitude axis\n",
    "data_all_sub_invlon = data_all_sub.reindex(lon=data_all_sub.lon[::-1])\n",
    "data_all_sub_invlon.coords[\"lon\"] = data_all_sub[\"lon\"]\n",
    "# generate data-array with flipped latitude and longitude axis\n",
    "data_all_sub_invlatlon = data_all_sub_invlat.reindex(lon=data_all_sub_invlat.lon[::-1])\n",
    "data_all_sub_invlatlon.coords[\"lon\"] = data_all_sub[\"lon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8548060-001d-492e-85f9-39ebd7f880c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for correctness of data augmentation \n",
    "var=\"t2m_in\"\n",
    "\n",
    "assert data_all_sub_invlat[var][0,-6,5].values == data_all_sub[var][0,5,5].values, \\\n",
    "       \"Latitude flipping did not work as expected. Check previous flipping method.\"\n",
    "assert data_all_sub_invlon[var][0, 5, -6].values == data_all_sub[var][0, 5, 5].values, \\\n",
    "       \"Longitude flipping did not work as expected. Check previous flipping method.\"\n",
    "assert data_all_sub_invlatlon[var][0, -6, -6].values == data_all_sub[var][0, 5, 5].values, \\\n",
    "       \"Latitude-Longitude flipping did not work as expected. Check previous flipping method.\"\n",
    "\n",
    "print(\"Checks for data augmentation have been passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d9e75-c68d-4839-8636-346b31b16803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate time-coordinates of flipped datasets for later merging\n",
    "data_all_sub_invlat.coords[\"time\"] = pd.to_datetime(times.values) + dt.timedelta(minutes=1)\n",
    "data_all_sub_invlon.coords[\"time\"] = pd.to_datetime(times.values) + dt.timedelta(minutes=2)\n",
    "data_all_sub_invlatlon.coords[\"time\"] = pd.to_datetime(times.values) + dt.timedelta(minutes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83ab96-fc4d-4dd0-b71f-5a2cb3c834c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataset along time-axis\n",
    "ds_aug = xr.concat([data_all_sub, data_all_sub_invlat, data_all_sub_invlon, data_all_sub_invlatlon], dim=\"time\")\n",
    "# print to check dimensions\n",
    "print(ds_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bf8f3-f78a-481e-8185-b55cc5adcf8e",
   "metadata": {},
   "source": [
    "Now that we have the complete, augmented dataset. Let's spilt up into training, validataion and testing data.\n",
    "The former comprises all data for the years 2016 to 2019, i.e. four years. The latter two subsets use data from 2020 with the months May, July and August for the validation and April, June and September for the test dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965434a-831f-4edb-bb6d-4532e0bbaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_train = list(range(2016,2020)) \n",
    "yr_val= yr_test = [2020]\n",
    "mo_val = [5, 7, 8]\n",
    "mo_test = [4, 6, 9]\n",
    "\n",
    "ds_train = ds_aug.sel(time=ds_aug.time.dt.year.isin(yr_train))\n",
    "ds_val = ds_aug.sel(time=(ds_aug.time.dt.month.isin(mo_val) & ds_aug.time.dt.year.isin(yr_val)))\n",
    "ds_test = ds_aug.sel(time=(ds_aug.time.dt.month.isin(mo_test) & ds_aug.time.dt.year.isin(yr_test)))\n",
    "\n",
    "# just a check taht we get all data\n",
    "assert len(ds_test[\"time\"]) + len(ds_val[\"time\"]) + len(ds_train[\"time\"]) == len(ds_aug[\"time\"]), \\\n",
    "   \"Not all samples ({0:d}) from the augmented dataset have been used.\".format(len(ds_aug[\"time\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63266a5-bcd4-4a2e-8f58-e281bfdc3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, write augmented dataset to netCDF-file\n",
    "datafile_train = os.path.join(datadir, \"maelstrom-downscaling_train_aug.nc\")\n",
    "datafile_val = datafile_train.replace(\"_train_\", \"_val_\")\n",
    "datafile_test = datafile_train.replace(\"_train_\", \"_test_\")\n",
    "\n",
    "data_dict = {datafile_train: ds_train, datafile_val: ds_val, datafile_test: ds_test}\n",
    "\n",
    "for dfile, ds in data_dict.items():\n",
    "    if os.path.exists(dfile):\n",
    "        print(\"File '{0}' already exists. Remove the file if you would like to create a new one.\".format(datafile_augmented))\n",
    "    else:\n",
    "        print(\"Write augmented dataset to file '{0}'...\".format(dfile))\n",
    "        ds.to_netcdf(dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0b6ff-98e5-4db6-a02c-a383d67394e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a1fce-34be-41b9-a690-eab82eb82a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup_kernel_maelstrom",
   "language": "python",
   "name": "jup_kernel_maelstrom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
